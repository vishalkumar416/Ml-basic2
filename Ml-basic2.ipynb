{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c02e32d-5b24-4229-8826-7df5817d1dce",
   "metadata": {},
   "source": [
    "<p>--\r\n",
    "\r\n",
    "### **Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?**\r\n",
    "\r\n",
    "#### **Overfitting**\r\n",
    "Overfitting happens when a model learns the training data too well, capturing noise and fluctuations rather than general patterns. As a result, it performs well on the training set but poorly on new, unseen data.  \r\n",
    "**Consequences:**  \r\n",
    "- Poor generalization to new data  \r\n",
    "- High accuracy on training data but low accuracy on test data  \r\n",
    "\r\n",
    "**Mitigation Strategies:**  \r\n",
    "- Reduce model complexity  \r\n",
    "- Use more training data  \r\n",
    "- Apply regularization (L1, L2)  \r\n",
    "- Use dropout in neural networks  \r\n",
    "\r\n",
    "#### **Underfitting**  \r\n",
    "Underfitting happens when a model is too simple and fails to capture the underlying patterns in the data. It performs poorly on both training and test data.  \r\n",
    "**Consequences:**  \r\n",
    "- High bias and poor accuracy on both training and test data  \r\n",
    "- The model is unable to learn meaningful patterns  \r\n",
    "\r\n",
    "**Mitigation Strategies:**  \r\n",
    "- Increase model complexity (use more layers or features)  \r\n",
    "- Train for a longer duration  \r\n",
    "- Reduce regularization  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Q2: How can we reduce overfitting? Explain in brief.**  \r\n",
    "Overfitting can be reduced using the following techniques:\r\n",
    "\r\n",
    "1. **Cross-validation:** Use k-fold cross-validation to ensure the model generalizes well.  \r\n",
    "2. **Regularization:** Apply L1 (Lasso) or L2 (Ridge) regularization to penalize large weights.  \r\n",
    "3. **Dropout:** Randomly drop neurons during training in neural networks.  \r\n",
    "4. **More training data:** Adding more samples improves generalization.  \r\n",
    "5. **Early stopping:** Stop training when validation loss starts increasing.  \r\n",
    "6. **Pruning:** Reduce the complexity of decision trees.  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Q3: Explain underfitting. List scenarios where underfitting can occur in ML.**  \r\n",
    "Underfitting occurs when a model is too simple to learn the underlying structure of the data.  \r\n",
    "\r\n",
    "**Scenarios where underfitting can occur:**  \r\n",
    "- Using a linear model for complex, non-linear relationships.  \r\n",
    "- Training a neural network with too few layers or neurons.  \r\n",
    "- Insufficient training epochs leading to poor learning.  \r\n",
    "- Over-regularization (high lambda values in Ridge/Lasso).  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?**  \r\n",
    "\r\n",
    "The **bias-variance tradeoff** describes the balance between two sources of error:  \r\n",
    "\r\n",
    "- **Bias (Underfitting)**: Error due to overly simplistic assumptions in the model.  \r\n",
    "- **Variance (Overfitting)**: Error due to excessive sensitivity to training data noise.  \r\n",
    "\r\n",
    "**Effect on model performance:**  \r\n",
    "- **High bias â†’ Underfitting:** Poor performance on both training and test data.  \r\n",
    "- **High variance â†’ Overfitting:** Good training accuracy but poor test accuracy.  \r\n",
    "\r\n",
    "The goal is to find an optimal balance to minimize total error.  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?**  \r\n",
    "\r\n",
    "**Methods to detect overfitting:**  \r\n",
    "- High training accuracy but low validation/test accuracy.  \r\n",
    "- Increasing validation loss while training loss keeps decreasing.  \r\n",
    "- Large difference between train and test scores (high variance).  \r\n",
    "\r\n",
    "**Methods to detect underfitting:**  \r\n",
    "- Low training and test accuracy.  \r\n",
    "- Both training and validation loss are high.  \r\n",
    "- The model fails to capture patterns in the data.  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?**  \r\n",
    "\r\n",
    "| Aspect | Bias (Underfitting) | Variance (Overfitting) |\r\n",
    "|--------|----------------------|------------------------|\r\n",
    "| Definition | Model is too simple and fails to learn patterns | Model is too complex and captures noise |\r\n",
    "| Training Error | High | Low |\r\n",
    "| Test Error | High | High |\r\n",
    "| Cause | Oversimplification | Over-sensitivity to training data |\r\n",
    "| Example | Linear regression on non-linear data | Deep neural networks without regularization |\r\n",
    "\r\n",
    "Examples:  \r\n",
    "- **High Bias:** Linear regression on non-linear data.  \r\n",
    "- **High Variance:** Decision trees without pruning, deep neural networks without regularization.  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.**  \r\n",
    "\r\n",
    "**Regularization** is a technique to prevent overfitting by adding a penalty term to the loss function. It discourages complex models and large weight values.\r\n",
    "\r\n",
    "**Common Regularization Techniques:**  \r\n",
    "1. **L1 Regularization (Lasso)** â€“ Shrinks some weights to zero, effectively performing feature selection.  \r\n",
    "2. **L2 Regularization (Ridge)** â€“ Shrinks weights but does not eliminate them. Helps reduce overfitting.  \r\n",
    "3. **Dropout (for Neural Networks)** â€“ Randomly drops neurons during training to prevent reliance , upload it to GitHub, and share the repository link as required. Let me know if you need any clarifications! ðŸš€</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78599bba-bcfe-4676-b44a-4e3397284c84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
